{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "22/12/07 21:50:09 WARN Utils: Your hostname, DOUGPC resolves to a loopback address: 127.0.1.1; using 172.22.121.112 instead (on interface eth0)\n",
      "22/12/07 21:50:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/07 21:50:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fe5659ce4a0>\n"
     ]
    }
   ],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, column, desc, window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkGuide\").master(\"spark://DOUGPC.:7077\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/doug/ProjetosEstudo/PySpark/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#aqui é a construção de um dataframe reader\u001b[39;00m\n\u001b[1;32m      2\u001b[0m flightData15\u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSpark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:410\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m     path \u001b[39m=\u001b[39m [path]\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    411\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    412\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/doug/ProjetosEstudo/PySpark/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv"
     ]
    }
   ],
   "source": [
    "#aqui é a construção de um dataframe reader\n",
    "flightData15= spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"../Python/SparkDefGuide/d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizando um comando de ação para visualizar o conteúdo do documento\n",
    "#a função take irá puxar três linhas do DF e mostrar no resultado.\n",
    "#vale destacar que no fim, essa estrutrua exibida é um array de lista.\n",
    "\n",
    "flightData15.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entendendo o query plan do spark \n",
    "#utilizando a função explain\n",
    "flightData15.sort(\"Count\", descending=True).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData15.sort(\"Count\", ascending=False).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\n",
    "flightData15.sort(\"Count\", ascending = False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurando a qtd de partições para essa transformação\n",
    "#essa configuração não manipula a transformação, mas situações como partição e operações de shuffle.\n",
    "#o que fazemos aqui é dimensionar os recursos para a transformação física do arquivo.\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData15.sort(\"Count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flightData15' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#criando uma table a partir do dataframe utilizado\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#daqui, podemos utilizar SparkSQL para consultas\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mflightData15\u001b[49m\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflight_Data_2015\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flightData15' is not defined"
     ]
    }
   ],
   "source": [
    "#criando uma table a partir do dataframe utilizado\n",
    "#daqui, podemos utilizar SparkSQL para consultas\n",
    "\n",
    "flightData15.createOrReplaceTempView(\"flight_Data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "                   SELECT DEST_COUNTRY_NAME, count(1)\n",
    "                   FROM FLIGHT_DATA_2015\n",
    "                   GROUP BY DEST_COUNTRY_NAME\"\"\")\n",
    "dataFrameWay = flightData15\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .count()\n",
    "    \n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT MAX(COUNT) FROM FLIGHT_DATA_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "flightData15.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlMax = spark.sql(\"\"\"SELECT\n",
    "                   DEST_COUNTRY_NAME,\n",
    "                   SUM(COUNT) AS TOTAL_DESTINATION\n",
    "                   FROM FLIGHT_DATA_2015\n",
    "                   GROUP BY (DEST_COUNTRY_NAME)\n",
    "                   ORDER BY SUM(COUNT) DESC\n",
    "                   LIMIT 5\"\"\"\n",
    "                   )\n",
    "sqlMax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movendo o resultado acima para o dataframe\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData15\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"Count\")\\\n",
    "    .withColumnRenamed(\"sum(Count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData15\\\n",
    "    .groupby(\"DEST_COUNTRY_NAME\")\\\n",
    "        .sum(\"Count\")\\\n",
    "            .withColumnRenamed(\"sum(Count)\", \"destination_total\")\\\n",
    "                .sort(desc(\"destination_total\"))\\\n",
    "                    .limit(5)\\\n",
    "                        .show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalizando\n",
    "flightData15\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"Count\")\\\n",
    "    .withColumnRenamed(\"sum(Count)\",\"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "#esse código diz ao Spark O QUE FAZER. O que é muito melhor do que na construção com RDD.\n",
    "#create a dataframe \n",
    "\n",
    "dataDF = spark.createDataFrame([(\"brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), \n",
    "                                (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "\n",
    "#agrupando o nome, a idade e calculando a média\n",
    "avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")\n",
    "\n",
    "staticDataFrame.createOrReplaceGlobalTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame\\\n",
    "    .selectExpr(\"CustomerId\",\n",
    "                \"(UnitPrice * Quantity) as TotalCost\",\n",
    "                \"InvoiceDate\")\\\n",
    "    .groupby(\n",
    "        col(\"CustomerId\"), window(col(\"InvoiceDate\"),\"1 Day\"))\\\n",
    "    .sum(\"TotalCost\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leitura por streaming\n",
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option(\"maxFilePerTrigger\", 1)\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"/Spark-The-Definitive-Guide/data/reatil-data/by-day/*.csv\")\n",
    "    \n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando uma regra de negócio de compra por hora\n",
    "#utilizando streaming\n",
    "#como não tem ação, isso é só uma transformação\n",
    "\n",
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "    .selectExpr(\n",
    "        \"CustomerId\",\n",
    "        \"(UnitPrice * Quantity) as TotalCost\",\n",
    "        \"InvoiceDate\")\\\n",
    "    .groupBy(\n",
    "        col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "    .sum(\"TotalCost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'purchaseByCustomerPerHour' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#o nome atribuído nas funções da variável devem ser minúsculos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#no sparksession, só pode haver mais de uma query com o mesmo nome\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpurchaseByCustomerPerHour\u001b[49m\u001b[38;5;241m.\u001b[39mwriteStream\\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_Purchase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'purchaseByCustomerPerHour' is not defined"
     ]
    }
   ],
   "source": [
    "#o nome atribuído nas funções da variável devem ser minúsculos\n",
    "#no sparksession, só pode haver mais de uma query com o mesmo nome\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_Purchase\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
