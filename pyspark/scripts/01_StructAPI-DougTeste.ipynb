{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# configSpark = pyspark.SparkConf().\\\n",
    "#                 setAll([('spark.executor.memory', '4g'),\n",
    "#                         ('spark.executor.cores', '2'),\n",
    "#                         ('spark.cores.max', '2'),\n",
    "#                         ('spark.driver.memory', '4g')])\n",
    "\n",
    "# construa a sessão no mesmo notebook que importou\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"BlogPost\")\\\n",
    "    .master(\"spark://DOUGPC.:7077\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "    # .master(\"spark://DOUGPC.:7077\")\\\n",
    "    # .config(conf= configSpark)\\\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simples dataframe criado com pyspark\n",
    "#print01\n",
    "newDf = spark.range(1, 100)\n",
    "newDf.head(10)\n",
    "newDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print1.1\n",
    "#criação de dataframe \n",
    "datafrm = spark.createDataFrame([('A', 20),\\\n",
    "                                ('B', 30),\\\n",
    "                                ('C', 40),\\\n",
    "                                ('D', 70),\\\n",
    "                                ('A', 30),\\\n",
    "                                ('C', 60)],\n",
    "                                ['Prod', 'Valor'])\n",
    "datafrm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criação de um schema utilizando a linguagem Python.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#atribuir à uma variável os schemas que irão para os dataframes\n",
    "schemaFrame = StructType([StructField('Produto', StringType(), False),\\\n",
    "                            StructField('Categoria', StringType(), True),\\\n",
    "                            StructField('Valor', IntegerType() , False)])\n",
    "\n",
    "#criando schema com Spark SQL.\n",
    "schemaSQL = \"Produto STRING, Categoria STRING, Valor INT\"\n",
    "\n",
    "#agora basta criar um novo dataframe e passar o schema com parâmetro da função\n",
    "\n",
    "pythonFrame = spark.createDataFrame([('A', 'Alimento', 30),\\\n",
    "                                        ('B', 'Papelaria', 40),\\\n",
    "                                        ('C', 'Informatica', 30),\\\n",
    "                                        ('D', 'Papelaria', 40),\\\n",
    "                                        ('E', 'Alimento', 40),\\\n",
    "                                        ('F', 'Informatica', 100),\\\n",
    "                                        ('G', 'Papelaria', 80),\\\n",
    "                                        ('A', 'Alimento', 30),\\\n",
    "                                        ('D', 'Papelaria', 50),\\\n",
    "                                        ('C', 'Informatica', 50),\\\n",
    "                                        ('J', 'Bebida', 100)],\n",
    "                                    ['Produto', 'Categoria', 'Valor'], schemaFrame)\n",
    "\n",
    "frameSQL = spark.createDataFrame([('A', 'Alimento', 30),\\\n",
    "                                        ('B', 'Papelaria', 40),\\\n",
    "                                        ('C', 'Informatica', 30),\\\n",
    "                                        ('D', 'Papelaria', 40),\\\n",
    "                                        ('E', 'Alimento', 40),\\\n",
    "                                        ('F', 'Informatica', 100),\\\n",
    "                                        ('G', 'Papelaria', 80),\\\n",
    "                                        ('A', 'Alimento', 30),\\\n",
    "                                        ('D', 'Papelaria', 50),\\\n",
    "                                        ('C', 'Informatica', 50),\\\n",
    "                                        ('J', 'Bebida', 100)],\n",
    "                                    ['Produto', 'Categoria', 'Valor'], schemaSQL)\n",
    "pythonFrame.groupby('Categoria').sum('Valor').show()\n",
    "\n",
    "#Long também pode ser considerado INT no Spark.\n",
    "#ele escolhe qual datatype vai atribuir baseado no que for melhor para performance.\n",
    "print(pythonFrame.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quando tratamos de criar schema, O true or false diz respeito ao null\n",
    "#se tal coluna do dataframe poderá ter valor null ou não, assim como uma tabela.\n",
    "\n",
    "nullFrame = StructType([StructField('Produto', StringType(), False),\\\n",
    "                            StructField('Categoria', StringType(), True),\\\n",
    "                            StructField('Valor', FloatType(), False),\\\n",
    "                            StructField('Qntd', LongType(), False)])\n",
    "\n",
    "\n",
    "pythonFrame = spark.createDataFrame([('A', 'Alimento', 30, 3),\\\n",
    "                                        ('B', 'Papelaria', 40,4 ),\\\n",
    "                                        ('C', 'Informatica', 30, 10),\\\n",
    "                                        ('D', 'Papelaria', 40, 5),\\\n",
    "                                        ('E', 'Alimento', 40, 3),\\\n",
    "                                        ('F', 'Informatica', 100, 10),\\\n",
    "                                        ('G', 'Papelaria', 80, 3),\\\n",
    "                                        ('A', 'Alimento', 30, 5),\\\n",
    "                                        ('D', 'Papelaria', 50,3),\\\n",
    "                                        ('C', 'Informatica', 50,5 ),\\\n",
    "                                        ('J', 'Bebida', 100, 10),\\\n",
    "                                        ('I','',30, 3)],\n",
    "                                    ['Produto', 'Categoria', 'Valor', 'Qntd'], nullFrame)\n",
    "print(pythonFrame.printSchema())\n",
    "pythonFrame.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um frame com toDF\n",
    "#img03\n",
    "rangeFrame = spark.range(500).toDF('rFrame')\n",
    "rangeFrame.show()\n",
    "rangeFrame.select(rangeFrame['rFrame'] + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando dataframes com uma lista de linhas\n",
    "\n",
    "#IMG04\n",
    "from datetime import date, datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rowListFrame = spark.createDataFrame([\n",
    "    Row(a = 1, b = 2, c= 'String', d=date(2000, 1, 1)),\n",
    "    Row(a = 2, b = 3, c= 'String 2', d= date(2000, 1, 2)),\n",
    "    Row(a = 3, b = 4, c= 'String 3', d= date(2000, 1, 3))])\n",
    "\n",
    "rowListFrame.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando dataframe com schema explícito\n",
    "\n",
    "#img 05\n",
    "expSchemaFrame = spark.\\\n",
    "    createDataFrame([\n",
    "        (1, 2., 'String 1', date(2000, 1, 1)),\n",
    "        (2, 3., 'String 2', date(2000, 1, 2)),\n",
    "        (3, 4., 'String 3', date(2000, 1, 3))\n",
    "], schema= 'a int, b double, c string, d date')\n",
    "\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "#se chamar uma função não funciona\n",
    "#se utilizar print, funciona e exibe apenas uma linha\n",
    "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 1)\n",
    "print(expSchemaFrame)\n",
    "expSchemaFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exibindo o nome das colunas e informaçoes do dataframe\n",
    "#img05.1\n",
    "expSchemaFrame.columns\n",
    "\n",
    "#para não passar as colunas, utilizo o atributo columns com o dataframe\n",
    "#função describe para descrever o dataframe\n",
    "expSchemaFrame.select(expSchemaFrame.columns).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um dataframe a partir de um pandas dataframe\n",
    "#utilizando um dicionário para cada coluna, passar suas linhas\n",
    "\n",
    "#img06\n",
    "pandasFrame = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': [date(2000, 1, 1), date(2000, 1, 2), date(2000, 1, 3)]\n",
    "})\n",
    "\n",
    "criapandasFrame = spark.createDataFrame(pandasFrame)\n",
    "criapandasFrame.show(1)\n",
    "\n",
    "criapandasFrame.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando o RDD para criar dataframe\n",
    "#passamos uma lista com tuplas para criar o dataframe\n",
    "\n",
    "#img07\n",
    "rddFrame = spark.sparkContext.parallelize([\n",
    "    (1, 2., 'String 1', date(2000, 1, 1)),\n",
    "    (2, 3., 'String 2', date(2000, 1, 2)),\n",
    "    (3, 4., 'String 4', date(2000, 1, 3))\n",
    "])\n",
    "\n",
    "criarddFrame = spark.createDataFrame(rddFrame, schema= ['a', 'b', 'c'])\n",
    "criarddFrame.show(1)\n",
    "print(criarddFrame.printSchema())\n",
    "\n",
    "criarddFrame.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect retorna uma lista com as linhas do dataframe\n",
    "#uma Row é um registro dentro do dataframe\n",
    "#todo registro dentro do dataframe é do tipo Row.\n",
    "\n",
    "#veja o exemplo da função collect()\n",
    "rangeFrame.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MANIPULAÇÃO DO DATAFRAME\n",
    "\n",
    "#CRIANDO UM NOVO DATAFRAME\n",
    "\n",
    "produtoSchema = StructType([StructField('Produto', StringType(), False),\\\n",
    "                            StructField('Categoria', StringType(), False),\\\n",
    "                            StructField('Quantidade', IntegerType(), False),\\\n",
    "                            StructField('Preco', FloatType(), False)])\n",
    "\n",
    "produtoVendas = spark.createDataFrame([['Iogurte', 'Laticinios', 4, 5.0],\\\n",
    "                                        ['Leite', 'Laticinios', 10, 3.5],\\\n",
    "                                        ['Queijo', 'Laticinios', 4, 2.5],\\\n",
    "                                        ['Mussarela', 'Laticinios', 5, 6.0],\\\n",
    "                                        ['Frango', 'Carnes', 20, 15.0],\\\n",
    "                                        ['Patinho', 'Carnes', 10, 20.0],\\\n",
    "                                        ['Carré', 'Carnes', 10, 8.0],\\\n",
    "                                        ['Arroz', 'Graos', 20, 3.5],\\\n",
    "                                        ['Feijao', 'Graos', 10, 3.5],\\\n",
    "                                        ['Aveia', 'Cereais', 20, 4.0],\\\n",
    "                                        ['Farinha trigo', 'Cereais', 5, 4.0],\\\n",
    "                                        ['Lentilha', 'Graos', 40, 3.5],\\\n",
    "                                        ['Sabao', 'Limpeza', 6, 10.0],\\\n",
    "                                        ['Amaciante', 'Limpeza', 5, 6.0],\\\n",
    "                                        ['Detergente', 'Limpeza', 20, 2.0],\\\n",
    "                                        ['Cloro', 'Limpeza', 5, 3.5],\\\n",
    "                                        ['Pera', 'Frutas', 10, 5.0],\\\n",
    "                                        ['Uva', 'Frutas', 6, 4.0],\\\n",
    "                                        ['Limao', 'Frutas', 10, 4.0],\\\n",
    "                                        ['Banana', 'Frutas', 6, 5.5],\\\n",
    "                                        ['Cenoura', 'Legumes', 20, 4.95],\\\n",
    "                                        ['Beterraba', 'Legumes', 40, 6.0]], produtoSchema)\n",
    "\n",
    "print(produtoVendas.toPandas())\n",
    "produtoVendas.collect()\n",
    "produtoVendas.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, column\n",
    "\n",
    "# col(produtoVendas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "\n",
    "#acessando colunas\n",
    "\n",
    "#img08\n",
    "# produtoVendas.select('Produto', 'Quantidade').show(5)\n",
    "# produtoVendas.select(produtoVendas.Quantidade).show(5)\n",
    "\n",
    "#img08.1\n",
    "\n",
    "#o select permite selecionar colunas do dataframe\n",
    "#além, podemos criar expressões e atribuir nomes com a função alias.\n",
    "produtoVendas.select(   \n",
    "                        produtoVendas.Produto,\\\n",
    "                        produtoVendas.Quantidade,\\\n",
    "                        produtoVendas.Categoria,\\\n",
    "                        produtoVendas.Preco,\\\n",
    "                        (produtoVendas.Preco + produtoVendas.Preco *  0.10).alias('Preco_10%')).\\\n",
    "                    show(1)\n",
    "#img8.2                  \n",
    "#manipulação com withColumn\n",
    "#com withColumn, primeiro passa o alias da coluna e após, a expressão.\n",
    "produtoVendas.withColumn('Preco_10%',\\\n",
    "                         (produtoVendas.Preco + produtoVendas.Preco*0.10)).\\\n",
    "                    show(5)\n",
    "\n",
    "#img8.3 diferença entre withColumn e select.\n",
    "\n",
    "#img8.4\n",
    "#upper case na coluna\n",
    "produtoVendas.withColumn('UpperCatg',\\\n",
    "                    upper('Categoria')).show(2)\n",
    "\n",
    "#filtrando as colunas de um dataframe\n",
    "#img8.5\n",
    "produtoVendas.filter(produtoVendas.Categoria == \"Frutas\").toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produtoVendas.show()\n",
    "# produtoVendas.write.csv('ProdVendas',sep= ',', header=True, mode='overwrite')\n",
    "\n",
    "# spark.read.csv('/home/douglas/Desktop/ProjetosEstudo/Python/PySpark/ProdVendas', header= True, sep=',').show()\n",
    "\n",
    "# spark.read.csv('../PySpark/ProdVendas', header=True, sep= ',').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flySchema = StructType([StructField('DEST_COUNTRY_NAME', StringType(), True),\\\n",
    "                        StructField('ORIGIN_COUNTRY_NAME',StringType(), True),\\\n",
    "                        StructField('count', LongType(), False, metadata={'hello':'world'})])\n",
    "\n",
    "flyFrame = spark.read.format('json').\\\n",
    "                            schema(flySchema).\\\n",
    "                            load('../SparkDefGuide/data/flight-data/json/2015-summary.json')\n",
    "\n",
    "flyFrame = spark.read.\\\n",
    "                    schema(flySchema).\\\n",
    "                    json('../SparkDefGuide/data/flight-data/json/2015-summary.json')\n",
    "flyFrame.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
