{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, IntegerType, FloatType, DataType, DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "\n",
    "sourcepath = '/home/doug/ProjetosEstudo/Spark-The-Definitive-Guide/data/'\n",
    "lrnpath = '/home/doug/ProjetosEstudo/LearningSparkV2/databricks-datasets/learning-spark-v2'\n",
    "\n",
    "spark = SparkSession.\\\n",
    "                    builder.\\\n",
    "                    master('spark://DOUGPC.:7077').\\\n",
    "                    appName('SparkSQLSources').\\\n",
    "                    getOrCreate()\n",
    "                    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options do spark essas configurações valem para todas as importações\n",
    "\n",
    "#irá excluir arquivos corrompidos\n",
    "spark.sql(\" set spark.sql.files.ignoreCorruptFiles= True \") \n",
    "\n",
    "#irá ignorar arquivos faltantes, caso seja uma sequencia de arquivos\n",
    "spark.sql(\" set spark.sql.files.ignoreMissingFiles= True \")\n",
    "\n",
    "#lerá apenas o formato passado \n",
    "spark.sql(\" set spark.sql.files.pathGlobFilter= \"\"*.csv\"\" \") \n",
    "\n",
    "#fara lookup dos arquivos de modo ativo, mas ignora particionamento\n",
    "spark.sql(\" set spark.sql.files.recurssiveFileLookup= True\") \n",
    "\n",
    "##permite criar um timeframe dos arquivos armazenados.\n",
    "#lerá somente os que se enquadrarem no frame\n",
    "spark.sql(\" set spark.sql.files.modifiedBefore= \"\"2050-07-01T08:30:00\"\" and modifiedAfter=\"\"2050-06-01T08:30:00\"\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightschema = StructType([StructField('ORIGIN_COUNTRY_NAME', StringType(), False),\n",
    "                           StructField('DEST_COUNTRY_NAME', StringType(), False),\n",
    "                           StructField('count', IntegerType(), False)])\n",
    "\n",
    "#inferschema irá passar para o spark que ele deve ler e atribuir o datatype nas colunas da fonte\n",
    "#pode adicionar custo de processamento caso seja true\n",
    "\n",
    "framereader = spark.read.\\\n",
    "                        format('json').\\\n",
    "                        schema(flightschema).\\\n",
    "                        option('mode', \"FAILFAST\").\\\n",
    "                        option('inferSchema', False).\\\n",
    "                        load(f'{lrnpath}/flights/summary-data/json/2010-summary.json').\\\n",
    "                        withColumnRenamed('ORIGIN_COUNTRY_NAME', 'origin').\\\n",
    "                        withColumnRenamed('DEST_COUNTRY_NAME', 'destine').\\\n",
    "                        withColumnRenamed('count', 'flyquantity')\n",
    "\n",
    "framereader.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#renomear a coluna gera um novo dataframe e ele não sobrescreve o atual\n",
    "#assim, crie uma nova variável que 'recebera' as colunas renomeadas.\n",
    "jsonframe = spark.read.\\\n",
    "                        load(format='json',\\\n",
    "                                schema= flightschema,\\\n",
    "                                path= f'{lrnpath}/flights/summary-data/json/2010-summary.json',\\\n",
    "                                mode='failFast',\\\n",
    "                                inferSchema = False)\n",
    "  \n",
    "#só acontece se for em um statement separado da leitura do dataframe.                      \n",
    "jsonrenamed = jsonframe.withColumnRenamed('ORIGIN_COUNTRY_NAME', 'origin').\\\n",
    "                        withColumnRenamed('DEST_COUNTRY_NAME', 'destine').\\\n",
    "                        withColumnRenamed('count', 'flyquantity')\n",
    "            \n",
    "jsonrenamed.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lrnpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando direto do comando SQL.\n",
    "\n",
    "# readsql = spark.\\\n",
    "#             sql(\"\"\" \n",
    "#                 SELECT * \n",
    "#                 FROM \n",
    "#                 csv. `file:///home/doug/ProjetosEstudo/Spark-The-Definitive-Guide/data/bike-data/201508_station_data.csv`\n",
    "#                 \"\"\")\n",
    "\n",
    "# readsql.show(2)\n",
    "\n",
    "\n",
    "sumflyqtd = spark.\\\n",
    "                sql(\"\"\" \n",
    "                    SELECT sum(_c4) as totaldockcount\n",
    "                    FROM \n",
    "                    csv. \n",
    "                    `file:///home/doug/ProjetosEstudo/Spark-The-Definitive-Guide/data/bike-data/201508_station_data.csv`\n",
    "                    \"\"\")\n",
    "\n",
    "sumflyqtd.show(2)\n",
    "\n",
    "\n",
    "     \n",
    "groupquery = spark.\\\n",
    "                sql(\"\"\" \n",
    "                    SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total \n",
    "                    FROM\n",
    "                    parquet. \n",
    "                    `file:///home/doug/ProjetosEstudo/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet`\n",
    "                    GROUP BY ORIGIN_COUNTRY_NAME\n",
    "                    HAVING ORIGIN_COUNTRY_NAME = 'Russia' or ORIGIN_COUNTRY_NAME = 'United States'\n",
    "                    \"\"\")\n",
    "groupquery.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readprq = spark.sql(\"\"\" \n",
    "                    SELECT * FROM parquet.\n",
    "                    `file:///home/doug/ProjetosEstudo/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet` \"\"\").\\\n",
    "                filter(\"count > 10\").\\\n",
    "                filter(\"ORIGIN_COUNTRY_NAME = 'United States' \")\n",
    "                \n",
    "readprq.show(0)\n",
    "\n",
    "\n",
    "readprq = spark.sql(\"\"\" \n",
    "                    SELECT * \n",
    "                    FROM parquet.\n",
    "                    `file:///home/doug/ProjetosEstudo/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet` \"\"\").\\\n",
    "                where(\"count > 10\").\\\n",
    "                where(\"ORIGIN_COUNTRY_NAME = 'United States' \").\\\n",
    "                show(0, truncate= True)\n",
    "    \n",
    "#criando agregações\n",
    "#na agregação, primeiro é a coluna, depois a operação.\n",
    "groupedqtd = spark.sql(\"\"\" \n",
    "                    SELECT * \n",
    "                    FROM parquet.\n",
    "                    `file:///home/doug/ProjetosEstudo/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet` \"\"\").\\\n",
    "                groupBy('ORIGIN_COUNTRY_NAME').\\\n",
    "                agg({'count': 'sum'}).\\\n",
    "                withColumnRenamed('sum(count)', 'origin_qtd')\n",
    "                \n",
    "groupedqtd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedqtd = spark.sql(\"\"\" \n",
    "                    SELECT * FROM parquet.\n",
    "                    `file:///home/doug/ProjetosEstudo/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet` \"\"\").\\\n",
    "                groupBy('ORIGIN_COUNTRY_NAME').\\\n",
    "                agg({'count': 'sum'}).\\\n",
    "                withColumnRenamed('sum(count)', 'origin_qtd')\n",
    "                \n",
    "groupedqtd.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeschema = StructType([\n",
    "                         StructField('station_id', IntegerType(), False),\n",
    "                         StructField('name', StringType(), False),\n",
    "                         StructField('lat', FloatType(), False),\n",
    "                         StructField('long', FloatType(), False),\n",
    "                         StructField('dockcount', IntegerType(), False),\n",
    "                         StructField('landmark', StringType(), False),\n",
    "                         StructField('installation', StringType(), False) \n",
    "                         ])\n",
    "#failfast irá acusar falha caso alguma linha esteja corrompida.\n",
    "permimport = spark.read.format('csv').\\\n",
    "                         load(f'{sourcepath}/bike-data/201508_station_data.csv',\n",
    "                              mode='failFast',\n",
    "                              inferschema = False,\n",
    "                              schema=bikeschema,\n",
    "                              header= True)\n",
    "                                 \n",
    "permimport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img05.1 dropmalformed irá apagar as linhas que estão corrompidas D\n",
    "\n",
    "parquetschema = StructType([\n",
    "                            StructField('DEST_COUNTRY_NAME', StringType(), False),\n",
    "                            StructField('ORIGIN_COUNTRY_NAME', StringType(), False),\n",
    "                            StructField('count', IntegerType(), False)\n",
    "                            ])\n",
    "parquetopt = spark.\\\n",
    "               read.\\\n",
    "               format('parquet').\\\n",
    "               load(f'{sourcepath}/flight-data/parquet/2010-summary.parquet',\n",
    "                         mode='dropmalformed',\n",
    "                         schema = parquetschema)\n",
    "                        \n",
    "parquetopt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencecsv = spark.read.\\\n",
    "                        option(\"ignoreCorrupFiles\", \"true\").\\\n",
    "                        csv(f'{sourcepath}/flight-data/csv/',\\\n",
    "                                header= True,\n",
    "                                modifiedAfter='2011-01-01 23:59:59',     # type: ignore\n",
    "                                schema= flightschema)\n",
    "\n",
    "\n",
    "globcsv = spark.read.load(f'{sourcepath}/flight-data/csv/2010-summary.csv',\n",
    "                                format= 'csv',\\\n",
    "                                recursiveFileLookup= True,\\\n",
    "                                header= True)\n",
    "\n",
    "timecsv = spark.read.load(f'{sourcepath}/flight-data/csv/',\\\n",
    "                                pathGlobFilter= \"*.csv\",\\\n",
    "                                format='csv',\\\n",
    "                                header= True,\\\n",
    "                                modifiedBefore= '2011-01-01 23:59:59',\n",
    "                                schema= flightschema)\n",
    "\n",
    "\n",
    "                        # option(\"ignoreCorrputFiles\", True).\\\n",
    "                        # option(\"ignoreMissingFiles\", True).\\\n",
    "                        # option(\"pathGlobFilter\", \"*.csv\").\\\n",
    "                        # option(\"mode\", 'failFast').\\\n",
    "                        # option('modifiedBefore', '2011-01-01 23:59:59').\\\n",
    "                        # csv(f'{sourcepath}/flight-data/csv/*.csv',\\\n",
    "                        #         header= True)\n",
    "\n",
    "# sequencecsv.show(5)\n",
    "print(sequencecsv.count())\n",
    "print(globcsv.count())\n",
    "print(timecsv.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abaixo, tratarei de parquet \n",
    "#todos os campos quando importado de fonte parquet passam a aceitar nulos. São \"nullable true\"\n",
    "\n",
    "parqueframe = spark.\\\n",
    "                    read.\\\n",
    "                    parquet(f'{lrnpath}/sf-airbnb/sf-airbnb-clean-100p.parquet')\n",
    "\n",
    "parqueframe.schema\n",
    "\n",
    "parquetpath = f'{lrnpath}sf-airbnb/sf-airbnb-clean-100p.parquet'\n",
    "\n",
    "parqueframe.write.mode('overwrite').saveAsTable('tableparquet', mode= 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parqueframe.createOrReplaceTempView('airbnbview')\n",
    "#o problema é ter que passar o path do arquivo completo\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          \n",
    "          CREATE OR REPLACE TEMPORARY VIEW PRQ_VIEW\n",
    "            USING parquet\n",
    "            OPTIONS (\n",
    "                PATH '/home/doug/ProjetosEstudo/LearningSparkV2/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean-100p.parquet'\n",
    "            )\n",
    "          \n",
    "          \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" select * from prq_view\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetfr = spark.read.csv(f'{sourcepath}flight-data/csv/2010-summary.csv', inferSchema= True, header=True)\n",
    "\n",
    "flightfr = ((spark.\n",
    "                read.\n",
    "                format('csv').\n",
    "                load(f'{sourcepath}bike-data', inferSchema= True, header=True).\n",
    "                withColumnRenamed('Trip ID', 'trip_id').\n",
    "                withColumnRenamed('Duation', 'duration').\n",
    "                withColumnRenamed('Start Date', 'start_date').\n",
    "                withColumnRenamed('Start Station', 'start_station').\n",
    "                withColumnRenamed('Start Terminal', 'start_terminal').\n",
    "                withColumnRenamed('End Date', 'end_date').\n",
    "                withColumnRenamed('End Station', 'end_station').\n",
    "                withColumnRenamed('End Terminal', 'end_terminal').\n",
    "                withColumnRenamed('Bike #', 'bike_identity').\n",
    "                withColumnRenamed('Subscriber Type', 'sub_type').\n",
    "                withColumnRenamed('Zip Code', 'zip_code')\n",
    "                \n",
    "            ))\n",
    "\n",
    "#mergeschema\n",
    "#escrever os dois dataframes no mesmo diretório e fazer um mergeschema\n",
    "\n",
    "parquetfr.show(5)\n",
    "flightfr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightfr.write.parquet('/tmp/prq/df=1', mode='overwrite')\n",
    "parquetfr.write.parquet('/tmp/prq/df=2', mode= 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeparquet = ((\n",
    "                spark.\n",
    "                    read.\n",
    "                    option('mergeSchema', True).\n",
    "                    parquet('/tmp/prq')\n",
    "                ))\n",
    "\n",
    "#print merged parquet schema\n",
    "mergeparquet.printSchema()\n",
    "\n",
    "#write new dataframe to directory\n",
    "mergeparquet.write.parquet('/tmp/mergeprq/', mode = 'overwrite')\n",
    "\n",
    "#create new table with merged schema\n",
    "mergeparquet.write.saveAsTable('bike_trip_merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#querying merged parquet table\n",
    "spark.sql( \"\"\"  \n",
    "                select \n",
    "                    trip_id,\n",
    "                    start_date,\n",
    "                    end_date,\n",
    "                    dest_country_name as destine,\n",
    "                    origin_country_name as origin,\n",
    "                    df\n",
    "                from bike_trip_merged\n",
    "                where df = 2\n",
    "                \n",
    "        \"\"\").fillna('Not Av').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightfr.createOrReplaceTempView('flightview_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/19 14:58:46 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 11\n",
      "CSV file: file:///home/doug/ProjetosEstudo/Spark-The-Definitive-Guide/data/bike-data/201508_station_data.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#parquet is a default format \n",
    "\n",
    "spark.sql(\"\"\" \n",
    "          select * from flightview_parquet\n",
    "          \"\"\"\n",
    "          ).write.\\\n",
    "            mode('overwrite').\\\n",
    "            save('/tmp/flight')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
