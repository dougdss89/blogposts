{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType, DataType\n",
    "from pyspark.sql.functions import *\n",
    "from os.path import abspath\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "                    appName(name= 'SQL_SOURCE').\\\n",
    "                    master('local[*]').\\\n",
    "                    config(\"spark.driver.extraClassPath\",\"/opt/mssql-tools18/msjdbc/sqljdbc42.jar\").\\\n",
    "                    config(\"spark.executor.extraClassPath\",\"/opt/mssql-tools18/msjdbc/sqljdbc42.jar\").\\\n",
    "                    getOrCreate()\n",
    "                    \n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mssqlprops = {\n",
    "            'driver': 'com.microsoft.sqlserver.jdbc.SQLServerDriver',\n",
    "            'database': 'adventureworks2019',\n",
    "            'user': 'sparkuser',\n",
    "            'password': 'sparkadmin',\n",
    "            'url':'jdbc:sqlserver://172.28.160.1:65230;database=spark'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightdatapath = abspath('/home/doug/ProjetosEstudo/Spark-The-Definitive-Guide/data/flight-data/parquet/2010-summary.parquet/flight_parquet/*')\n",
    "retailpath = abspath('/home/doug/ProjetosEstudo/Spark-The-Definitive-Guide/data/retail-data/all/online-retail-dataset.csv')\n",
    "\n",
    "parquetschema = StructType([\n",
    "                            StructField('DEST_COUNTRY_NAME', StringType(), True),\n",
    "                            StructField('ORIGIN_COUNTRY_NAME', StringType(), True),\n",
    "                            StructField('count', IntegerType(), True)\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retailframe = spark.read.csv(path=retailpath, header=True)\n",
    "flightframe = spark.read.format('parquet').\\\n",
    "                            load(path=flightdatapath).\\\n",
    "                            withColumnRenamed('DEST_COUNTRY_NAME', 'destine').\\\n",
    "                            withColumnRenamed('ORIGIN_COUNTRY_NAME', 'origin').\\\n",
    "                            withColumnRenamed('count', 'flyquantity')\n",
    "retailframe.show(2)\n",
    "flightframe.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mssqlcon = \"jdbc:sqlserver://172.28.160.1:65230;database=spark\"\n",
    "\n",
    "df = spark.\\\n",
    "        read.\\\n",
    "        format('jdbc').\\\n",
    "        option('url', mssqlcon).\\\n",
    "        option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\").\\\n",
    "        option('user', 'sparkuser').\\\n",
    "        option('password', 'sparkadmin').\\\n",
    "        option('query', \n",
    "                \"\"\"select \n",
    "                    salesorderid as orderid, \n",
    "                    salesorderdetailid as qtdperorder,\n",
    "                    orderqty as qty,\n",
    "                    pt.productid,\n",
    "                    pt.name as productname\n",
    "                from salestable as st\n",
    "                    inner join\n",
    "                    productable as pt\n",
    "                on st.productid = pt.productid\n",
    "                \"\"\").\\\n",
    "    load()\n",
    "    \n",
    "df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletable = spark.\\\n",
    "                    read.\\\n",
    "                    format('jdbc').\\\n",
    "                    option('url', mssqlcon).\\\n",
    "                    option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\").\\\n",
    "                    option('user', 'sparkuser').\\\n",
    "                    option('password', 'sparkadmin').\\\n",
    "                    option('query', \n",
    "                            \"\"\"select top(100)\n",
    "                                SalesOrderID\n",
    "                                ,SalesOrderDetailID\n",
    "                                ,CarrierTrackingNumber\n",
    "                                ,OrderQty\n",
    "                                ,st.ProductID as pdid,\n",
    "                                name,\n",
    "                                productnumber,\n",
    "                                productsubcategoryid\n",
    "                            from salestable as st\n",
    "                                inner join\n",
    "                                productable as pt\n",
    "                            on st.productid = pt.productid\n",
    "                            order by newid() \n",
    "                            \"\"\").\\\n",
    "                load().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salestable = sampletable = spark.\\\n",
    "                    read.\\\n",
    "                    format('jdbc').\\\n",
    "                    option('url', mssqlcon).\\\n",
    "                    option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\").\\\n",
    "                    option('user', 'sparkuser').\\\n",
    "                    option('password', 'sparkadmin').\\\n",
    "                    option('query', \n",
    "                            \"\"\"\n",
    "                                select * from salestable\n",
    "                                \n",
    "                            \"\"\").\\\n",
    "                load()\n",
    "# salestable.distinct().explain()\n",
    "# salestable.filter('productid in (729, 733)').explain()\n",
    "salestable.filter('productid in (729, 733)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salestable.filter('productid in (729, 733)').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionframe =((\n",
    "    spark.\\\n",
    "        read.\\\n",
    "        format('jdbc').\\\n",
    "        option('url', mssqlcon).\\\n",
    "        option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\").\\\n",
    "        option('user', 'sparkuser').\\\n",
    "        option('password', 'sparkadmin').\\\n",
    "        option('dbtable', 'salestable').\\\n",
    "        option('numPartitions', 10).\\\n",
    "        option('partitionColumn', 'salesorderid').\\\n",
    "        option('colname', 'salesorderid').\\\n",
    "        option('lowerBound', '100').\\\n",
    "        option('upperBound','1000').\\\n",
    "    load()\n",
    "))\n",
    "\n",
    "partitionframe.count()\n",
    "\n",
    "# partitionframe.write.saveAsTable('partsales', format= 'parquet', mode= 'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectpartsales = ((\n",
    "    \n",
    "                spark.\n",
    "                sql(\"\"\" select * from partsales \"\"\")\n",
    "                \n",
    "))\n",
    "\n",
    "selectpartsales.count()\n",
    "\n",
    "selectpartsales.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#essa forma de leitura não é a mais adequada\n",
    "#somente uma query por consulta.\n",
    "testesales = (\n",
    "            spark.\n",
    "            read.\n",
    "            jdbc(url = mssqlcon,\n",
    "                table = 'person.person',\n",
    "                table =  'sales.salesterritory',\n",
    "                properties = mssqlprops)).show(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#criar tabela no banco\n",
    "partitionframe.write.\\\n",
    "            jdbc(mssqlcon, 'salesprod', 'overwrite',\n",
    "                properties = {'user':'sparkuser', \n",
    "                            'password':'sparkadmin',\n",
    "                            'driver': 'com.microsoft.sqlserver.jdbc.SQLServerDriver'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
