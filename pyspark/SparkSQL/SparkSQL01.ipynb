{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from  pyspark.sql.catalog import Catalog\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO SESSÃO\n",
    "\n",
    "warehousePath = abspath('spark_database')\n",
    "sparkv2path = '/home/doug/ProjetosEstudo/LearningSparkV2/databricks-datasets/learning-spark-v2'\n",
    "sparkdefguidepath = '/home/doug/ProjetosEstudo/Spark-The-Definitive-Guide'\n",
    "master= \"local[*]\"\n",
    "worker=\"spark://DOUGPC.:7077\"\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName('SparkSQL').\\\n",
    "        master(master).\\\n",
    "        config(\"spark.sql.warehouse.dir\", warehousePath).\\\n",
    "        config(\"spark.sql.catalogImplementation\", \"hive\").\\\n",
    "        config(\"spark.sql.legacy.createHiveTableByDefault\", \"false\").\\\n",
    "        enableHiveSupport().\\\n",
    "        getOrCreate()\n",
    "        \n",
    "print(warehousePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RODANDO UMA QUERY EM SQL COM PYTHON\n",
    "df = spark.sql(\"\"\"select 1+1\"\"\")\n",
    "df.show(1)\n",
    "parquetPath = (f'{sparkdefguidepath}/data/flight-data/parquet/2010-summary.parquet')\n",
    "\n",
    "print(sparkdefguidepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaFlight = StructType([\n",
    "                            StructField('DEST_COUNTRY_NAME', StringType(), nullable= False),\n",
    "                            StructField('ORIGIN_COUNTRY_NAME', StringType(), nullable= False),\n",
    "                            StructField('count', IntegerType(), nullable= False)])\n",
    "\n",
    "flightframe = spark.read.csv(f'{sparkdefguidepath}/data/flight-data/csv/2010-summary.csv', schema= schemaFlight, header=True, sep=',').\\\n",
    "                        withColumnRenamed('DEST_COUNTRY_NAME', 'destine').\\\n",
    "                        withColumnRenamed('ORIGIN_COUNTRY_NAME', 'origin').\\\n",
    "                        withColumnRenamed('count', 'flyqtd')\n",
    "\n",
    "#img01 erro dataframe  com sql                      \n",
    "flightframe = spark.sql(\"\"\"\n",
    "                        SELECT \n",
    "                            origin,\n",
    "                            flyqtd\n",
    "                        FROM flightframe\n",
    "                        where flyqtd > 100\"\"\")\n",
    "\n",
    "flightframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img02 criando uma view temporária e executando uma consulta sql                        \n",
    "flightframe.createOrReplaceTempView('flightview')\n",
    "\n",
    "viewFrame = spark.sql(\"\"\"\n",
    "                select origin, flyqtd\n",
    "                from flightview\n",
    "                where flyqtd * 2 > flyqtd\"\"\")\n",
    "\n",
    "viewFrame.show(3)\n",
    "\n",
    "#img02.1 group by com pyspark \n",
    "viewFrame.groupBy('origin').\\\n",
    "            sum('flyqtd').\\\n",
    "            show(3)\n",
    "\n",
    "\n",
    "            \n",
    "#manipulando um dataframe com SQL e com a API PySpark\n",
    "\n",
    "doubleWhrere = spark.sql(\"\"\"\n",
    "                        select \n",
    "                            ORIGIN,\n",
    "                            FLYQTD\n",
    "                        FROM FLIGHTVIEW\n",
    "                        WHERE FLYQTD > 100\"\"\").\\\n",
    "                    where(\"origin like 'I%'\").\\\n",
    "                    where(\"flyqtd > 200\")\n",
    "\n",
    "doubleWhrere.show(3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "windowFrame = spark.sql(\"\"\"\n",
    "                        select \n",
    "                            origin,\n",
    "                            flyqtd,\n",
    "                            row_number() over (order by origin) as rownum\n",
    "                        from flightview\n",
    "                        where flyqtd > 100\"\"\")\n",
    "windowFrame.show(3)\n",
    "\n",
    "\n",
    "\n",
    "partitionFrame = spark.sql(\"\"\"\n",
    "                            SELECT\n",
    "                                ORIGIN,\n",
    "                                FLYQTD,\n",
    "                                SUM(FLYQTD) OVER (PARTITION BY ORIGIN\n",
    "                                                    ORDER BY FLYQTD\n",
    "                                                    ROWS UNBOUNDED PRECEDING) as SumPerOrigin\n",
    "                            FROM FLIGHTVIEW\"\"\")\n",
    "partitionFrame.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayFlightSchema = StructType([\n",
    "                                StructField('date', StringType(), nullable= False),\n",
    "                                StructField('delay', IntegerType(), nullable= False),\n",
    "                                StructField('distance', IntegerType(), nullable= False),\n",
    "                                StructField('origin', StringType(), nullable= True),\n",
    "                                StructField('destination', StringType(), nullable= False)])\n",
    "\n",
    "\n",
    "flightDelayPath = spark.read.csv(f'{sparkv2path}/flights/departuredelays.csv', header= True, schema= delayFlightSchema)\n",
    "\n",
    "flightDelayPath.createOrReplaceTempView('tempflight')\n",
    "\n",
    "flightViewNoReplace = spark.read.csv(f'{sparkv2path}/flights/departuredelays.csv', header= True, schema= delayFlightSchema)\n",
    "\n",
    "# flightViewNoReplace.createTempView('noreplaceflight')\n",
    "\n",
    "castDate = spark.sql(\"\"\"\n",
    "                    SELECT\n",
    "                        DATE,\n",
    "                        SUBSTRING(DATE, 0, 2) AS SUB,\n",
    "                        SUBSTRING(DATE, 3, 2) AS SUB2,\n",
    "                        DELAY,\n",
    "                        ORIGIN,\n",
    "                        DESTINATION\n",
    "                    FROM TEMPFLIGHT\n",
    "                    ORDER BY DELAY DESC\"\"\")\n",
    "\n",
    "concatview = castDate.createOrReplaceTempView('castDate')\n",
    "\n",
    "# concatws = spark.sql(\"\"\"\n",
    "#                      select\n",
    "#                         sub,\n",
    "#                         sub2,\n",
    "#                         concat_ws('-', sub, sub2) as date\n",
    "#                     from castDate\"\"\")\n",
    "# concatws.show(2)\n",
    "\n",
    "tempFrame = castDate.select(concat_ws('-', castDate.SUB, castDate.SUB2).alias('DATE'),\n",
    "                castDate.DELAY,\n",
    "                castDate.ORIGIN,\n",
    "                castDate.DESTINATION)\n",
    "\n",
    "tempFrame.createOrReplaceTempView('usflight')\n",
    "tempFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando uma classificação com SQL CASE\n",
    "\n",
    "classfication = spark.sql(\"\"\"\n",
    "                            SELECT\n",
    "                                DATE, ORIGIN, DESTINATION, DELAY, \n",
    "                                CASE\n",
    "                                    WHEN DELAY > 360 THEN 'VERY LONG'\n",
    "                                    WHEN DELAY >= 120 AND DELAY < 360 THEN 'LONG DELAY'\n",
    "                                    WHEN DELAY >= 60 AND DELAY < 120 THEN 'SHORT DELAY'\n",
    "                                    WHEN DELAY > 0 AND DELAY < 60 THEN 'TOLERABLE'\n",
    "                                    WHEN DELAY = 0 THEN 'NO DELAY'\n",
    "                                ELSE 'EARLY'\n",
    "                            END AS US_DELAY\n",
    "                            FROM usflight\"\"\")\n",
    "\n",
    "classfication.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando cte com case\n",
    "classfication = spark.sql(\"\"\"\n",
    "                            with class_flight as (\n",
    "                                select\n",
    "                                    DATE, ORIGIN, DESTINATION, DELAY, \n",
    "                                    CASE\n",
    "                                        WHEN DELAY > 360 THEN 'VERY LONG'\n",
    "                                        WHEN DELAY >= 120 AND DELAY < 360 THEN 'LONG DELAY'\n",
    "                                        WHEN DELAY >= 60 AND DELAY < 120 THEN 'SHORT DELAY'\n",
    "                                        WHEN DELAY > 0 AND DELAY < 60 THEN 'TOLERABLE'\n",
    "                                        WHEN DELAY = 0 THEN 'NO DELAY'\n",
    "                                    ELSE 'EARLY'\n",
    "                                END AS US_DELAY\n",
    "                                FROM usflight)\n",
    "                                \n",
    "                            select * from class_flight\n",
    "                            where US_DELAY LIKE \"TOLE%\"\n",
    "                            \"\"\")\n",
    "classfication.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempFrame.select('DELAY', 'ORIGIN', 'DESTINATION').\\\n",
    "            where('DELAY > 1500').\\\n",
    "            orderBy('DELAY', ascending= False).show()\n",
    "            \n",
    "tempFrame.select(concat('DELAY', 'DESTINATION').\\\n",
    "                    alias('DELAY_DEST'), 'ORIGIN').\\\n",
    "                show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMG06\n",
    "\n",
    "tempFrame.select('DELAY', 'ORIGIN').\\\n",
    "            orderBy('ORIGIN', ascending= False).\\\n",
    "            groupBy('ORIGIN').\\\n",
    "            sum('DELAY').\\\n",
    "        show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql( \"\"\" drop database if exists testedb\"\"\")\n",
    "\n",
    "spark.sql (\"\"\" CREATE DATABASE testeDB \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO TABELAS GERENCIADAS\n",
    "\n",
    "#img07\n",
    "\n",
    "spark.sql(\"\"\"DROP DATABASE IF EXISTS flightdb CASCADE\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE DATABASE flightdb\"\"\")\n",
    "\n",
    "spark.sql(\"\"\" USE flightdb \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" DROP TABLE IF EXISTS FLIGHTTABLE\"\"\")\n",
    "\n",
    "pathSource = f'{sparkv2path}/flights/departuredelays.csv'\n",
    "\n",
    "flightTableSchema = StructType([\n",
    "                                StructField(\"date\", StringType(), False),\n",
    "                                StructField(\"delay\", IntegerType(), False),\n",
    "                                StructField(\"distance\", IntegerType(), False),\n",
    "                                StructField(\"origin\", StringType(), False),\n",
    "                                StructField(\"destination\", StringType(), False)])\n",
    "\n",
    "flightTable = spark.read.csv(path=pathSource,\n",
    "                             header= True,\n",
    "                             schema= flightTableSchema)\n",
    "\n",
    "#criando uma tabela gerenciada.\n",
    "#utiliza o método write antes do saveAsTable\n",
    "flightTable.write.saveAsTable(name=\"FlightTable\", mode=\"overwrite\")\n",
    "\n",
    "selectTable = spark.sql(\"\"\"\n",
    "                        SELECT * FROM FLIGHTTABLE \"\"\")\n",
    "\n",
    "selectTable.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMG 7.1 TABELA GERENCIADA COM SPARK.SQL\n",
    "#IMG 7.2 PRINT DOS ARQUIVOS NO DIRETÓRIO.\n",
    "\n",
    "spark.sql(\"\"\"USE flightdb \"\"\")\n",
    "\n",
    "#para criar tabelas gerenciadas, armazenando tanto os dados quanto os metadados\n",
    "\n",
    "spark.sql(\"\"\" DROP TABLE IF EXISTS USFLIGHT \"\"\")\n",
    "\n",
    "spark.sql (\"\"\"\n",
    "            \n",
    "            CREATE TABLE USFLIGHT(\n",
    "                \n",
    "                DATE STRING,\n",
    "                DELAY INT,\n",
    "                DISTANCE INT,\n",
    "                ORIGIN STRING,\n",
    "                DESTINY STRING\n",
    "                \n",
    "            )\"\"\")#.explain(mode= 'formatted')\n",
    "#inserindo\n",
    "spark.sql(\"\"\" \n",
    "          INSERT INTO USFLIGHT (DATE, DELAY, DISTANCE, ORIGIN, DESTINY)\n",
    "          VALUES (1111111, 111111, 111111, 'AAAAA', 'BBBBB')\"\"\")\n",
    "\n",
    "#consultando\n",
    "spark.sql(\"\"\"\n",
    "          SELECT *\n",
    "          FROM USFLIGHT \n",
    "          WHERE ORIGIN LIKE 'AAA%'\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO UMA TABELA NÃO GERENCIADA\n",
    "#VEJA QUE UTILIZO O USING COM O FORMATO DO ARQUIVO E AS OPÇÕES.\n",
    "#COMO SE ESTIVESSE UTILIZANDO A PYSPARK API.\n",
    "\n",
    "#IMG08\n",
    "#IMG08.1 PRINT COM O PATH DO SPARK_DATABASE SEM A TABELA CRIADA.\n",
    "spark.sql(\"\"\" USE flightdb \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" drop table if exists sales_table\"\"\")\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "        \n",
    "        CREATE TABLE SALES_TABLE (\n",
    "            INVOICE_NO STRING,\n",
    "            STOCKCODE STRING,\n",
    "            DESCRIPTION STRING,\n",
    "            QUANTITY INT,\n",
    "            INVOICEDATE STRING,\n",
    "            UNITPRICE DECIMAL,\n",
    "            CUSTOMERID INT,\n",
    "            COUNTRY STRING)\n",
    "           \n",
    "        USING csv OPTIONS (header true, \n",
    "        PATH '/home/doug/ProjetosEstudo/Spark-The-Definitive-Guide/data/retail-data/all/online-retail-dataset.csv') \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" SELECT * FROM SALES_TABLE\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" drop table if exists teste_table\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE TESTE_TABLE \n",
    "          USING parquet OPTIONS\n",
    "          (PATH \"/home/doug/ProjetosEstudo/Spark-The-Definitive-Guide/data/flight-data/parquet/2010-summary.parquet\")\"\"\")\n",
    "\n",
    "spark.sql (\"select * from teste_table\"\"\").\\\n",
    "        explain(mode= \"formatted\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parquetFrame = spark.read.parquet(f\"{sparkdefguidepath}/data/flight-data/parquet/2010-summary.parquet\").\\\n",
    "                        withColumnRenamed(\"DEST_COUNTRY_NAME\", \"destine\").\\\n",
    "                        withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"origin\")\n",
    "                        \n",
    "parquetFrame.createOrReplaceTempView(\"prqFlight\")\n",
    "\n",
    "# spark.sql(\"\"\" select * from prqflight\"\"\").show(2)\n",
    "spark.sql(\"\"\" USE flightdb \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" drop table if exists flight\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            CREATE TABLE FLIGHT               \n",
    "            USING parquet \n",
    "            PARTITIONED BY (destine) AS \n",
    "            SELECT * \n",
    "            FROM prqFlight\n",
    "            ORDER BY ORIGIN\n",
    "            \n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" select * from flight\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO TABELA GERENCIADA COM CREATE EXTERNAL TABLE\n",
    "\n",
    "spark.sql( \"\"\" drop table if exists external_table \"\"\")\n",
    "\n",
    "spark.sql( \"\"\" \n",
    "            create external table external_table\n",
    "            row format delimited fields terminated by ','\n",
    "            location '/home/doug/ProjetosEstudo/PySpark/SparkSQL/spark_database/external/'\n",
    "            as select * from flight \"\"\").\\\n",
    "        explain(mode= 'formatted')\n",
    "\n",
    "spark.sql (\"\"\" select * from external_table \"\"\").\\\n",
    "        explain(mode= 'formatted')\n",
    "        \n",
    "# spark.sql(\"\"\" select * from external_table \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIANDO TABELAS COM \"SELECT INTO\"\n",
    "#DEVE SER CRIADA A PARTIR DE UMA VIEW OU TABLE EXISTENTE\n",
    "#LEMBRANDO QUE É UMA TABELA GERENCIADA\n",
    "\n",
    "#tempflight é uma view criada no início deste notebook\n",
    "\n",
    "spark.sql(\"\"\" drop table if exists dtwtable\"\"\")\n",
    "\n",
    "spark.sql(\"\"\" CREATE TABLE DTWTABLE \n",
    "                SELECT * FROM tempflight \n",
    "                WHERE destination = 'DTW' \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesTable = spark.sql( \"\"\" select * from sales_table \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" DROP TABLE IF EXISTS SALESGREATER10\"\"\")\n",
    "\n",
    "#IMG08.2.1 CRIANDO UMA TABELA GERENCIADA UTILIZANDO PYSPARK + DATAFRAME\n",
    "salesTable.select('INVOICE_NO','QUANTITY', 'UNITPRICE', 'CUSTOMERID').\\\n",
    "            where(\"COUNTRY like 'United%'\").\\\n",
    "            where(\"QUANTITY >= 10\").\\\n",
    "        write.\\\n",
    "            saveAsTable(\"salesGreater10\")\n",
    "            \n",
    "#consultando a tabela            \n",
    "spark.sql(\"\"\" SELECT * FROM SALESGREATER10\"\"\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flightframe.show(1)\n",
    "\n",
    "spark.sql(\"\"\" drop table if exists explain_table\"\"\")\n",
    "\n",
    "flightframe.write.saveAsTable('explain_table')\n",
    "\n",
    "spark.sql(\"\"\"  select * from explain_table \"\"\").\\\n",
    "        explain(mode= \"formatted\")\n",
    "        \n",
    "print(warehousePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TABELA PARTICIONADA\n",
    "\n",
    "#CRIANDO UMA TABELA PARTICIONADA A PARTIR DE UMA TABELA N-GERENCIADA\n",
    "#IMG8.3 TABELA PARTICIONADA A PARTIR DE UMA TABELA N-GERENCIADA.\n",
    "\n",
    "# spark.sql(\"\"\" DROP TABLE IF EXISTS partSALESTABLE \"\"\")\n",
    "\n",
    "# spark.sql(\"\"\" CREATE TABLE partSALESTABLE\n",
    "#           PARTITIONED BY (CUSTOMERID)\n",
    "#           AS \n",
    "#           SELECT *\n",
    "#           FROM SALES_TABLE \"\"\")\n",
    "\n",
    "# spark.sql(\"\"\" \n",
    "#             SELECT * \n",
    "#             FROM partSALESTABLE\n",
    "#             WHERE CUSTOMERID IS NOT NULL\"\"\").\\\n",
    "#       show(1)\n",
    "      \n",
    "#PRINT 8.4 CRIANDO VIEW PARTICIONADA VIA WINDOW FUNCTION\n",
    "spark.sql( \"\"\"  \n",
    "           SELECT \n",
    "            INVOICE_NO,\n",
    "            STOCKCODE,\n",
    "            QUANTITY,\n",
    "            CUSTOMERID,\n",
    "           row_number() OVER(PARTITION BY CUSTOMERID\n",
    "                              ORDER BY CUSTOMERID\n",
    "                              ROWS UNBOUNDED PRECEDING) AS RN\n",
    "           FROM SALES_TABLE \"\"\").\\\n",
    "      createOrReplaceTempView('partview')\n",
    "\n",
    "spark.sql(\"\"\" select * from partview \"\"\").\\\n",
    "      show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print 8.5 utilizando describe \n",
    "\n",
    "#print 8.5.1 criando a tabela e consultando\n",
    "spark.sql(\"\"\" describe table flight \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" drop table if exists metadata_salesgreater \"\"\")\n",
    "\n",
    "spark.sql( \"\"\" describe table salesgreater10 \"\"\").\\\n",
    "        write.\\\n",
    "        saveAsTable('metadada_salesgreater', mode= 'append')\n",
    "        \n",
    "spark.sql(\"\"\" select * from metadada_salesgreater \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print 8.5.2 partição, atualização dos metadados e reparo das tabelas\n",
    "\n",
    "spark.sql(\"\"\" show partitions partsalestable \"\"\").show(2)\n",
    "\n",
    "spark.sql(\"\"\" refresh table usflight \"\"\").show(5)\n",
    "\n",
    "spark.sql(\"\"\" msck repair table partsalestable \"\"\").show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache table\n",
    "#print 8.6 cache table\n",
    "spark.sql(\"\"\" cache table partsalestable \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" uncache table partsalestable \"\"\")\n",
    "\n",
    "#print 8.6.1 cache table na criação\n",
    "spark.sql(\"\"\" drop table if exists cache_table \"\"\")\n",
    "\n",
    "spark.sql(\"\"\" create table cache_table\n",
    "                           SELECT \n",
    "            INVOICE_NO,\n",
    "            STOCKCODE,\n",
    "            QUANTITY,\n",
    "            CUSTOMERID,\n",
    "           row_number() OVER(PARTITION BY CUSTOMERID\n",
    "                              ORDER BY CUSTOMERID\n",
    "                              ROWS UNBOUNDED PRECEDING) AS RN\n",
    "           FROM SALES_TABLE \"\"\"\n",
    "                                ).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" show tables in  flightdb \"\"\").show()\n",
    "\n",
    "#execute os comandos abaixo para excluir os databases, tabelas e views.\n",
    "# spark.sql(\"\"\" drop database if exists flight_db cascade \"\"\")\n",
    "\n",
    "# spark.sql(\"\"\" drop database if exists teste_db cascade \"\"\")\n",
    "\n",
    "# spark.sql(\"\"\" drop table if exists external \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\" drop table cache_table \"\"\")\n",
    "# spark.sql(\"\"\" drop table dtwtable \"\"\")\n",
    "# spark.sql(\"\"\" drop table explain_table \"\"\")\n",
    "# spark.sql(\"\"\" drop table flight \"\"\")\n",
    "# spark.sql(\"\"\" drop table flighttable \"\"\")\n",
    "# spark.sql(\"\"\" drop table metadata_salesgreater \"\"\")\n",
    "# spark.sql(\"\"\" drop table partsalestable \"\"\")\n",
    "# spark.sql(\"\"\" drop table salesgreater10 \"\"\")\n",
    "# spark.sql(\"\"\" drop table usflight \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
