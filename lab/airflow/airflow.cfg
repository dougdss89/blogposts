[core]
dags_folder = /opt/airflow/dags
hostname_callable = socket.getfqdn
might_contain_dag_callable = airflow.utils.file.might_contain_dag_via_default_heuristic
default_timezone = utc
executor = CeleryExecutor
parallelism = 32
max_active_tasks_per_dag = 16
dags_are_paused_at_creation = True
max_active_runs_per_dag = 16
load_examples = False
plugins_folder = /opt/airflow/plugins
execute_tasks_new_python_interpreter = False
task_runner = StandardTaskRunner
xcom_backend = airflow.models.xcom.BaseXCom
lazy_load_plugins = True
lazy_discover_providers = True
donot_pickle = True
dagbag_import_timeout = 30.0
dagbag_import_error_tracebacks = True
dagbag_import_error_traceback_depth = 2
dag_file_processor_timeout = 50
default_pool_task_slot_count = 128

[database]
sql_alchemy_conn = postgresql+psycopg2://user:password@postgres/dbname
sql_engine_encoding = utf-8
sql_alchemy_pool_enabled = True
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
sql_alchemy_pool_recycle = 1800
sql_alchemy_pool_pre_ping = True
load_default_connections = True
max_db_retries = 3
check_migrations = True

[logging]
base_log_folder = /opt/airflow/logs
remote_logging = False
remote_log_conn_id =
delete_local_logs = False
logging_level = INFO
fab_logging_level = WARNING
colored_console_log = True
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
dag_processor_log_target = file
task_log_reader = task
worker_log_server_port = 8793
file_task_handler_new_folder_permissions = 0o775
file_task_handler_new_file_permissions = 0o664
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index >= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log

[webserver]
base_url = http://localhost:8181
web_server_host = 0.0.0.0
web_server_port = 8181
secret_key = WNSVlc7sgXIupbwGh5pyhA==
workers = 4
worker_class = sync
expose_config = True

[celery]
celery_app_name = airflow.executors.celery_executor
worker_concurrency = 16
broker_url = redis://redis:6379/0
flower_host = 0.0.0.0
flower_port = 5555
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

[scheduler]
job_heartbeat_sec = 3
scheduler_heartbeat_sec = 3
num_runs = -1
orphaned_tasks_check_interval = 300.0
child_process_log_directory = /opt/airflow/logs/scheduler

[triggerer]
default_capacity = 1000