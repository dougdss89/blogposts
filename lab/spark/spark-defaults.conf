# Configuração Básica do Spark
spark.master=spark://spark-master:7077
spark.eventLog.enabled=true
spark.eventLog.dir=/opt/spark/spark-events
spark.history.ui.port=18080
spark.history.fs.logDirectory=file:/opt/spark/spark-events

# Hadoop Configuration
spark.hadoop.fs.defaultFS=hdfs://nodemaster.hdp.com:9000
spark.hadoop.dfs.nameservices=ha-cluster
spark.hadoop.dfs.ha.namenodes.ha-cluster=nn1,nn2
spark.hadoop.dfs.namenode.rpc-address.ha-cluster.nn1=nodemaster.hdp.com:9000
spark.hadoop.dfs.namenode.rpc-address.ha-cluster.nn2=secondnode.hdp.com:9000
spark.hadoop.dfs.client.failover.proxy.provider.ha-cluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
spark.hadoop.dfs.replication 1

# SQL and Warehouse Configuration
spark.sql.warehouse.dir=hdfs://ha-cluster/sparkdw
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.write.compression.codec=snappy
parquet.block.size=134217728  
parquet.page.size=1048576  
spark.sql.files.maxPartitionBytes=134217728  
spark.sql.files.openCostInBytes=4194304  
spark.sql.sources.partitionOverwriteMode=dynamic 
spark.sql.files.maxRecordsPerFile=1000000
spark.sql.shuffle.partitions=200
spark.sql.hive.metastorePartitionPruning=true
spark.sql.parquet.filterPushdown=true
spark.sql.parquet.mergeSchema=false
spark.sql.autoBroadcastJoinThreshold=10485760
spark.sql.broadcastTimeout=300
spark.sql.execution.arrow.pyspark.enabled=true
hive.metastore.warehouse.dir=hdfs://ha-cluster/sparkdw

# Memory and Executor Configuration
spark.executor.memory=4g  
spark.driver.memory=4g  
spark.executor.cores=2  
spark.driver.cores=2  
spark.executor.memoryOverhead=1g  
spark.executor.instances=4  
spark.driver.maxResultSize=4g

# Dynamic Allocation
spark.dynamicAllocation.enabled         true  
spark.dynamicAllocation.minExecutors    2
spark.dynamicAllocation.maxExecutors    8  
spark.dynamicAllocation.initialExecutors 4  

# JVM Options
# Opções JVM
# Configuração única para driver e executor
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseStringDeduplication -Dlog4j.configuration=file:/opt/spark/conf/log4j.properties
spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:+UseStringDeduplication -Dlog4j.configuration=file:/opt/spark/conf/log4j.properties

# Shuffle Configuration
spark.shuffle.compress=true          
spark.shuffle.spill.compress=true    
spark.shuffle.file.buffer=32k       
spark.shuffle.io.maxRetries=5       
spark.shuffle.io.retryWait=5s        

# Iceberg Configuration
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.iceberg_table=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg_table.type=hadoop
spark.sql.catalog.iceberg_table.uri=hdfs://ha-cluster/iceberg_dw
spark.sql.catalog.iceberg_table.warehouse=hdfs://ha-cluster/iceberg_dw
spark.sql.catalog.iceberg_table.io-impl=org.apache.iceberg.hadoop.HadoopFileIO
spark.sql.defaultCatalog=iceberg_table
spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,com.github.ben-manes.caffeine:caffeine:3.1.8,org.apache.parquet:parquet-hadoop-bundle:1.12.3

# Define o nível padrão para WARN
log4j.rootCategory=WARN, console

# Ajuste logs específicos para pacotes Spark
log4j.logger.org.apache.spark=WARN
log4j.logger.org.spark-project=WARN
log4j.logger.org.apache.hadoop=WARN
log4j.logger.io.netty=WARN
log4j.logger.org.apache.jetty=WARN

# Configuração do console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
log4j.logger.org.apache.spark=WARN
log4j.logger.org.apache.hadoop=WARN
log4j.logger.org.apache.iceberg=WARN



